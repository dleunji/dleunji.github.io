---
Title : "언어모델"
layout : post
date : 2021-04-24
category : Python
blog : true
author : dleunji
description : 언어모델
---



## NLP

### 언어모델

언어라는 현상을 모델링하고자 단어 시퀀스(또는 문장)에 확률을 할당하는 모델

- 통계를 이용한 방법

- 인공신경망을 이용한 방법. - GPT, BERT

- 가장 보편적인 방법 : 이전 단어들이 주어졌을 때 다음 단어를 예측

- 확률 할당

  - 기계 번역
  - 오타 교정
  - 음성 인식

- 주어진 이전 단어들로부터 다음 단어 예측하기

  - 단어 시퀀스의 확률

    - 하나의 단어를 w, 단어 시퀀스를 대문자 W라고 한다면, n개의 단어가 등장하는 단어 시퀀스 W의 확률
    - P(W)=P(w1,w2,w3,w4,w5,...,wn)

  - 다음 단어 등장 확률

    - n - 1개의 단어가 나열된 상태에서 n번째. 단어의 확률

    - P(wn|w1,...,wn−1)

    - 조건부 확률

    - 전체 단어 시퀀스 W의 확률은 모든 단어가 예측되고 나서야 알 수 있으므로 단어 시퀀스의 확률은...

      <img width="428" alt="스크린샷 2021-04-25 오후 12 24 23" src="https://user-images.githubusercontent.com/46207836/115979560-8f841000-a5c1-11eb-9ff4-88463b15d837.png">

- 통계적 언어 모델(SLM)

  - p(B|A)=P(A,B)/P(A)
  - P(A,B)=P(A)P(B|A)
  - P(A,B,C,D)=P(A)P(B|A)P(C|A,B)P(D|A,B,C)
  - 조건부 확률의 연쇄 법칙

- 이를 기반으로 문장에 대한 확률

  - <img width="428" alt="스크린샷 2021-04-25 오후 12 24 23" src="https://user-images.githubusercontent.com/46207836/115979560-8f841000-a5c1-11eb-9ff4-88463b15d837.png">
  - P(An adorable little boy is spreading smiles)=
    P(An)×P(adorable|An)×P(little|An adorable)×P(boy|An adorable little)×P(is|An adorable little boy)×P(spreading|An adorable little boy is)×P(smiles|An adorable little boy is spreading)
  - 카운트 기반의 접근
    - 희소 문제
    - 예를 들어 위와 같이 P(is|An adorable little boy)를 구하는 경우에서 기계가 훈련한 코퍼스에 An adorable little boy is라는 단어 시퀀스가 없었다면 이 단어 시퀀스에 대한 확률은 0이 된다.
    - 또는 An adorable little boy라는 단어 시퀀스가 없었다면 분모가 0이 되어 확률은 정의되지 않습니다. 그렇다면 코퍼스에 단어 시퀀스가 없다고 해서 이 확률을 0 또는 정의되지 않는 확률이라고 하는 것이 정확한 모델링 방법일까? No!
    - 언어적 모델의 트렌드는 통계적 언어 모델에서 인공 신경망 언어 모델로 넘어간다.
  - N-gram 언어 모델
    1. 코퍼스에서 카운트하지 못하는 경우의 감소
       - P(is|An adorable little boy)≈ P(is|boy)
       - 카운트 확률 증가
    2. N-gram
       - 이 때 임의의 개수를 정하기 위한 기준을 위해 사용한다.
       - n개의 연속적인 단어 나열
    3. 한계
       - 만약 앞 내용으로 뒤의 내용이 정반대가 되어야 자연스러운 것이라면?
       - 희소문제는 여전히 존재
       - 적용 분야(Domain)에 맞는 코퍼스 수집

- Perplexity

  - 모델 내에서 자신의 성능을 수치화하여 결과를 내놓는 내부 평가(Intrinsic evaluation)

  - PPL이 낮을수록 언어 모델의 성능이 좋다.

  - PPL은 단어의 수로 정규화된 테스트 데이터에 대한 확률의 역수

  - 즉 PPL 최소화는 곧 문장의 확률 최대화

    <img width="715" alt="스크린샷 2021-04-25 오후 12 49 29" src="https://user-images.githubusercontent.com/46207836/115980059-b1cb5d00-a5c4-11eb-8888-8dc3d06bfecb.png">

- 분기계수(Branching factor)

  - PPL은 선택할 수 있는 가능한 경우의 수를 의미
  - 단, 평가 방법에 있어서 주의할 점은 PPL의 값이 낮다는 것은 테스트 데이터 상에서 높은 정확도를 보인다는 것이지, 사람이 직접 느끼기에 좋은 언어 모델이라는 것을 반드시 의미하진 않는다. 
  - 또한 언어 모델의 PPL은 테스트 데이터에 의존하므로 두 개 이상의 언어 모델을 비교할 때는 정량적으로 양이 많고, 또한 도메인에 알맞은 동일한 테스트 데이터를 사용해야 신뢰도가 높다.

## 토픽 모델링

### 잠재 의미 분석(Latent Semantic Analysis)

정확히는 토픽 모델링을 위해 최적화된 알고리즘은 아니지만, 중요한 아이디어이다. 

BoW에 기반한 DTM이나 TF-IDF는 기본적으로 단어의 빈도 수를 이용한 수치화 방법이다. 하지만 단어의 의미를 고려하지 못한다. 이를 위한 대안으로 DTM의 잠재된 의미를 이끌어주는 방법으로 LSA이라는 방법이 있다.

 - 특이값 분해 (Singular Value Decomposition, SVD)

   - 실수 벡터 공간에 한정하여 내용을 설명함을 명시

   - A가 m * n행렬일 때, 다음과 같이 3개의 행렬의 곱으로 분해하는 것

   - A=UΣVT

   - 여기서 각 3개의 행렬은 다음과 같은 조건 만족

     - U = m * m 직교행렬

     - V = n * n 직교행렬
     - Σ = m * n 직사각 대각행렬
     - 직교행렬이란 자신과 자신의 전치 행렬의 곱 또는 이를 반대로 곱한 결과가 단위행렬이 되는 행렬
     - 대각 행렬이란 주대각선을 제외한 곳의 원소가 모두 0

   - 이 때 SVD로 나온 대각 행렬의 대각 원소의 값을 행렬 A의 특이값(single value)

   - full sad 말고도, 일부 벡터들을 삭제시킨 절단된 svd(truncated SVD) 사용

   <img width="353" alt="스크린샷 2021-04-25 오후 6 39 45" src="https://user-images.githubusercontent.com/46207836/115988643-a3496980-a5f5-11eb-9c9c-447a8edd3b10.png">

   절단된 SVD는 대각 행렬 Σ의 대각 원소의 값 중에서 상위값 t개만 남게 된다. 또한 U행렬, V행렬의 t열까지만 남긴다.

### 2. 잠재 디리클레 할당(Latent Dirichlet Allocation)

문서의 집합으로부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘

앞서 배운 빈도수 기반의 표현 방법인 BoW의 행렬 DTM 똔느 TF-IDF 행렬을 입력으로 한다.

→ LDA는 단어의 순서는 신경쓰지 않는다

### 가정

1. 문서에 사용할 단어의 개수 N을 정한다.

- 5개의 단어를 정하였다.

2. 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정한다.

- 위 예제와 같이 토픽이 2개라고 하였을 때 강아지 토픽을 60%, 과일 토픽을 40%와 같이 선택할 수있다.

3. 문서에 사용할 각 단어를 정한다.

   ① 토픽 분포에서 토픽 T를 확률적으로 고른다.

   e.g. 60% 확률로 강아지 토픽을 선택하고, 40% 확률로 과일 토픽을 선택할 수 있다.

   ② 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고른다.

   e.g. 강아지 토픽을 선택하였다면, 33% 확률로 강아지란 단어를 선택할 수 있다.

   ③ 1,2 반복

### 수행과정

1. 사용자는 알고리즘에게 토픽의 개수 k를 알려준다.

2. 모든 단어를 k개 중 하나의 토픽에 할당한다.

3. 반복

   어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당되어져 있지만, 다른 단어들은 전부 올바른 토픽에 할당되어져 있는 상태라고 가정한다. 이에 따라 단어 w는 아래의 두가지 기준에 따라서 토픽이 재할당된다.

   \- p(topic t | document d) : 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율
   \- p(word w | topic t) : 각 토픽들 t에서 해당 단어 w의 분포

---

LSA : DTM을 차원 축소하여 축소 차원에서 근접 단어들을 토픽으로 묶는다.

LDA : 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합 확률로 추정하여 토픽을 추출한다.





